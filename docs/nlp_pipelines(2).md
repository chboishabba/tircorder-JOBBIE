# NLP Pipeline Processing Rules

This document describes how SensibLaw converts legal text into structured rule atoms and how upcoming semantic inference layers will attach ontology concepts. It separates currently implemented stages from pending semantic layers.

---

## 1. End-to-end NLP pipeline (status overview)

The target pipeline flows through the following stages:

1. Sentence segmentation âœ… (implemented)
2. Clause decomposition âœ… (implemented)
3. Syntactic argument extraction âœ… (implemented)
4. ActorRole â†’ ActorClass mapping ğŸš§ (planned)
5. Predicate interpretation â†’ Modality extraction âœ… (implemented)
6. Interest-bearing entity identification ğŸš§ (planned)
7. ProtectedInterest inference ğŸš§ (planned)
8. ValueFrame inference ğŸš§ (planned)
9. WrongType candidate inference ğŸš§ (planned)
10. RuleAtom generation âœ… (implemented)
11. Binding â†’ LegalSource ğŸš§ (planned)
12. Storage â†’ DB ontology layer âœ… (implemented for RuleAtoms)

Implemented layers are detailed in Â§2â€“Â§5. Planned semantic layers and their expected contracts are defined in Â§6â€“Â§10 with action items for delivery.

---

## 2. spaCy pipeline (implemented)

The spaCy pipeline underpins tokenisation, entity recognition, and rule harvesting. Its processing rules are implemented across the pipeline modules and matcher configuration files.

### 2.1 Token stream construction

* `SpacyAdapter` loads `en_core_web_sm` (or a blank English pipeline) with the named entity recogniser disabled, providing deterministic tokenisation even when pre-trained weights are unavailable. Each emitted token preserves surface text, lemmas, coarse POS tags, dependency labels, and entity types, while whitespace-only tokens are dropped.ã€F:src/pipeline/tokens.pyâ€ L1-L78ã€‘
* The shared adapter is exposed via `get_spacy_adapter()` so downstream code reuses a single cached instance with NER disabled, ensuring consistent segmentation across calls.ã€F:src/pipeline/tokens.pyâ€ L80-L103ã€‘

### 2.2 Sentence segmentation and lemmatisation

* The higher-level adapter in `src/nlp/spacy_adapter.py` attempts to load `en_core_web_sm` with NER disabled, falling back to `spacy.blank("en")` as needed. It guarantees sentence boundaries by adding a `sentencizer` when the pipeline lacks parsing components, and initialises a lookup lemmatiser where possible so every token exposes a lemma even when statistical resources are missing.ã€F:src/nlp/spacy_adapter.pyâ€ L18-L61ã€‘ã€F:src/nlp/spacy_adapter.pyâ€ L63-L92ã€‘
* `parse()` enforces string inputs, ensures the chosen pipeline can emit sentences, and serialises each sentence span into `{text,start,end,tokens}` records where every token carries text, lemma, POS, dependency label, and character offsets. The helper collapses the pipeline output into `{text, sents}` for downstream consumers.ã€F:src/nlp/spacy_adapter.pyâ€ L94-L140ã€‘

### 2.3 Legal named-entity enrichment

* `_ensure_entity_ruler()` guarantees an `EntityRuler` component is inserted (before `ner` when present), configures it to preserve existing entities, and hydrates it from `patterns/legal_patterns.jsonl`. Each pattern provides labelled templates for references to Acts, cases, and provisions.ã€F:src/pipeline/ner.pyâ€ L33-L82ã€‘ã€F:patterns/legal_patterns.jsonlâ€ L1-L4ã€‘
* The custom `reference_resolver` component merges rule-based spans (recorded under `doc.spans['REFERENCE']`) and statistical entities with labels in `REFERENCE`, `PERSON`, `ORG`, or `LAW`. It de-duplicates overlapping spans and normalises the `reference_source` extension so downstream consumers can trace whether a hit came from the pattern set, an entity ID, or the entity label itself.ã€F:src/pipeline/ner.pyâ€ L15-L128ã€‘
* `configure_ner_pipeline()` appends the resolver when missing, while `get_ner_pipeline()` caches the configured `Language` object so repeated calls reuse the same spaCy model and legal patterns.ã€F:src/pipeline/ner.pyâ€ L130-L150ã€‘

### 2.4 Dependency harvesting (syntactic argument extraction)

* `_load_pipeline()` tries to load the small/medium/large English pipelines that ship with spaCy, raising a runtime error with installation guidance if none provide a dependency parser. This ensures dependency-based rules only run when parser weights are installed.ã€F:src/rules/dependencies.pyâ€ L24-L78ã€‘
* `_collect_candidates()` iterates each sentence, normalises supported dependency arcs (e.g. coercing `dobj` to `obj` and `root` verbs to `verb`), extracts span text for argument roles, and deduplicates `DependencyCandidate` entries per label. Only arcs from `_SUPPORTED_DEPS` survive, keeping the downstream rule matcher focused on subject/object/complement style relations.ã€F:src/rules/dependencies.pyâ€ L80-L173ã€‘
* `get_dependencies()` orchestrates parsing, sentence iteration, and candidate aggregation, returning a `SentenceDependencies` list that buckets dependency roles by sentence text.ã€F:src/rules/dependencies.pyâ€ L195-L229ã€‘

### 2.5 Rule matcher configuration

* `src/nlp/rules.py` registers a shared `Matcher` keyed by spaCy `Vocab` objects. The pattern table covers modalities (`must`, `shall`, `may`, plus negative variants), conditional connectors (`if`, `unless`, `provided that`, etc.), references (sections, parts, Acts), and penalty phrases. Matches are greedily resolved so the longest applicable span wins.ã€F:src/nlp/rules.pyâ€ L1-L89ã€‘ã€F:src/nlp/rules.pyâ€ L99-L125ã€‘
* Normalisation helpers collapse matched spans into canonical enums. `Modality.normalise()` and `ConditionalConnector.normalise()` convert free-text spans into controlled identifiers, removing duplicates, stripping trailing punctuation, and trimming clause markers so downstream logic trees receive clean modality, condition, reference, and penalty buckets.ã€F:src/nlp/rules.pyâ€ L127-L199ã€‘
* `match_rules()` executes the matcher, deduplicates values per semantic role, and returns a `RuleMatchSummary` containing ordered modality choices, canonical conditions, normalised references, and penalties. The first modality becomes `primary_modality`, giving downstream components a deterministic default.ã€F:src/nlp/rules.pyâ€ L201-L266ã€‘

### 2.6 Normalisation rules feeding the pipeline

* Prior to spaCy processing, `normalise()` rewrites domain terminology via the glossary, lowercases the result, and constructs lightweight `Token` objects with guessed POS, lemma, and morphological features. Each token carries a configurable `token._.class_` extension to support later logic-tree labelling. These deterministic guesses provide a fallback when spaCy resources are unavailable, ensuring every pipeline stage receives well-formed token objects.ã€F:src/pipeline/__init__.pyâ€ L1-L208ã€‘

---

## 3. Rule extraction and atom assembly (implemented)

The rule-to-atom workflow is implemented in `src/rules/extractor.py`, `src/rules/__init__.py`, and `src/pdf_ingest.py`.

### 3.1 Sentence scanning and pattern matching

1. `_split_sentences()` scans provision text once, buffering characters until it reaches `.`, `;`, or a newline that is not inside parentheses. This preserves citations such as â€œ(1992) 175 CLR 1â€ so they remain intact for later reference parsing.ã€F:src/rules/extractor.pyâ€ L44-L86ã€‘
2. Each sentence is normalised and fed to `_PATTERNS`, a list containing `_NORMATIVE_PATTERN` and `_OFFENCE_PATTERN`. The first captures â€œactor + modality (must/may/shall) + restâ€; the second recognises â€œactor commits/is guilty of offence if/when/by restâ€, preserving the offence label as part of the modality.ã€F:src/rules/extractor.pyâ€ L9-L36ã€‘ã€F:src/rules/extractor.pyâ€ L199-L238ã€‘
3. If a sentence opens with `if/when/where/unless`, `_normalise_condition_text()` trims the leading clause and stores it as a preliminary condition before pattern matching begins, so prefixed conditions like â€œIf the Minister is satisfiedâ€¦â€ are not treated as actors.ã€F:src/rules/extractor.pyâ€ L120-L196ã€‘ã€F:src/rules/extractor.pyâ€ L166-L198ã€‘
4. When `_OFFENCE_PATTERN` matches, the offence label is appended to the modality (e.g. â€œcommits the offence of theftâ€) and the trigger word (`if/when/by`) is reinserted before the remainder of the clause, retaining offence semantics for downstream categorisation.ã€F:src/rules/extractor.pyâ€ L238-L252ã€‘

### 3.2 Conditional, scope, and element extraction

5. After a match, `_normalise_condition_text()` strips trailing â€œthenâ€ markers and compresses whitespace, ensuring condition fragments are canonicalised before storage.ã€F:src/rules/extractor.pyâ€ L110-L118ã€‘ã€F:src/rules/extractor.pyâ€ L256-L277ã€‘
6. If the matched `rest` contains a nested `if/when/unless`, the substring before that marker becomes the action, and the remainder becomes an additional condition, which is merged with any prefix captured in step 3. `scope` clauses beginning with â€œwithinâ€ or â€œunderâ€ are cut from the action so that spatial/authority limits can be handled separately.ã€F:src/rules/extractor.pyâ€ L254-L279ã€‘
7. `_classify_fragments()` decomposes the action, conditions, and scope into offence elements: it detects exception phrases, fault/mental state terms, result clauses, circumstance modifiers, and remaining conduct text. Each fragment is cleaned with `_clean_fragment()` to drop punctuation, deduplicated case-insensitively, and assigned to roles such as `conduct`, `fault`, `circumstance`, `exception`, and `result`. Scope fragments and inline `if/when` clauses are also treated as `circumstance` entries so downstream logic has a unified bucket.ã€F:src/rules/extractor.pyâ€ L118-L161ã€‘ã€F:src/rules/extractor.pyâ€ L162-L219ã€‘

### 3.3 Party classification

8. `derive_party_metadata()` (from `src/rules/__init__.py`) normalises the actor string by lowercasing and stripping non-alphabetic characters, then consults the curated taxonomy in `data/ontology/actors.yaml`. Each taxonomy entry defines a canonical `role`, human-readable `who_text`, and a list of aliases; `_match_party()` tests whether any alias appears as a standalone token inside the actor text. If the actor is not found, deterministic fallbacks infer a â€œdefence/accusedâ€ party for phrases like â€œany personâ€ or modalities such as â€œcommits/is guilty ofâ€. Otherwise, the party defaults to `unknown`, signalling that a lint should be raised later.ã€F:src/rules/__init__.pyâ€ L13-L138ã€‘ã€F:data/ontology/actors.yamlâ€ L1-L81ã€‘

### 3.4 Rule construction

9. Every successful pattern match becomes a `Rule` dataclass instance holding `actor`, `modality`, `action`, `conditions`, `scope`, and the classified `elements`. The derived `party`, `role`, and `who_text` are stored alongside these attributes so later steps know how to describe the actor even when the raw text lacks detail.ã€F:src/rules/extractor.pyâ€ L280-L320ã€‘
10. `_rules_to_atoms()` (in `src/pdf_ingest.py`) consumes the `Rule` list. For each rule it:
    * Copies actor/modality/action text and removes inline parenthetical citations via `_strip_inline_citations()`, capturing any recognised case citations as `RuleReference` objects with `work`, `section`, `pinpoint`, and `citation_text` metadata.ã€F:src/pdf_ingest.pyâ€ L1239-L1302ã€‘ã€F:src/pdf_ingest.pyâ€ L1303-L1354ã€‘
    * Reconstructs a combined `text` string consisting of actor, modality, action, conditions, and scope, then strips citations again to prevent duplicate references in the resulting atom.ã€F:src/pdf_ingest.pyâ€ L1334-L1369ã€‘
    * Instantiates a `RuleAtom` with the canonical party metadata (`party`, `role`, `who_text`) returned by step 8, and stores the condition/scope strings so clients can render them without re-running the regex matcher.ã€F:src/pdf_ingest.pyâ€ L1347-L1380ã€‘
    * Converts each offence element fragment (conduct, fault, circumstance, exception, result) into a `RuleElement` labelled as `atom_type="element"`, copying any citations that were attached to the fragment and linking the fragment to glossary candidates via `GlossaryLinker`. Circumstance fragments inherit the parent ruleâ€™s conditions when appropriate so the linkage carries the full context.ã€F:src/pdf_ingest.pyâ€ L1381-L1408ã€‘
    * Records structured references by linking `RuleReference` instances to glossary entries when possible. This produces machine-readable citations in `rule_atom.references` and eventual legacy atom `refs` values.ã€F:src/pdf_ingest.pyâ€ L1409-L1419ã€‘
    * Emits `RuleLint` entries when `party == UNKNOWN_PARTY`, flagging atoms that still need manual actor classification. Lints are stored beside the rule and later flattened into the legacy atom view with `atom_type="lint"`.ã€F:src/pdf_ingest.pyâ€ L1411-L1417ã€‘

11. `_rules_to_atoms()` returns a list of `RuleAtom` objects that feed directly into each provision. When a provision is serialised or loaded, `Provision.ensure_rule_atoms()` backfills any missing representations: it converts legacy `Atom` rows into structured `RuleAtom`s (preserving `refs`, `glossary_id`, and metadata) and flattens structured rule atoms back into the legacy schema when needed. This compatibility layer guarantees that every stored provision exposes both the rich structured form and the historical â€œatomâ€ view used by search indices.ã€F:src/pdf_ingest.pyâ€ L1417-L1420ã€‘ã€F:src/models/provision.pyâ€ L573-L760ã€‘

### 3.5 Legacy atom flattening

12. `RuleAtom.to_atoms()` maps each structured rule to one or more legacy `Atom` records: the subject atom mirrors the ruleâ€™s actor/modality/action text; each `RuleElement` becomes a derived atom tagged with its element role; and any `RuleLint` produces a `lint` atom referencing the offending rule. This deterministic flattening is invoked whenever `Provision.sync_legacy_atoms()` runs, ensuring the SQLite compatibility view (`atoms`) always lines up with the structured representation stored across `rule_atoms`, `rule_atom_elements`, `rule_atom_references`, and related tables.ã€F:src/models/provision.pyâ€ L400-L582ã€‘

### 3.6 Legal-BERT pipeline note

A dedicated Legal-BERT pipeline is not currently implemented. There are no Legal-BERT model wrappers, transformer dependencies, or configuration files to documentâ€”`pyproject.toml` and `requirements.txt` list spaCy and other classical NLP dependencies but omit any Hugging Face or BERT packages.ã€F:pyproject.tomlâ€ L10-L36ã€‘ã€F:requirements.txtâ€ L1-L19ã€‘ Additions should be recorded alongside the spaCy summary when available.

---

## 4. Planned semantic layers and ontology alignment

The following sections describe the intended contracts for upcoming semantic layers. Each includes expected inputs/outputs and action items for implementation.

### 4.1 ActorRole â†’ ActorClass mapping

**Purpose:** Map raw rule actor phrases into ontology concepts (`ActorClass`, `RoleMarker`, `RelationshipKind`) so downstream semantics know what sort of actor the rule addresses.

**Inputs:**

- `Rule` / `RuleAtom` fields (`party`, `role`, `who_text`, dependency-based subject spans)
- Named entities (`PERSON`, `ORG`, `LAW`; references to public bodies, courts, agencies)
- Optional external metadata (case parties, legal source metadata)

**Outputs:**

- `actor_class_id` (e.g., `private.person`, `state.agency`, `community.collective`, `religious.officer`, `corporate.entity`)
- Optional `role_marker_ids` and `relationship_kind_ids` for constraints embedded in the rule
- Provenance: `source = "rule_actor_classifier_v1"`

**Action items:**

- Implement classifier that maps `RuleAtom.party`/`who_text` + NER context to `ActorClass` and optional relationship markers (Ticket: NLP-AC-001).
- Surface unresolved actors as structured lints so they are reviewable in the UI/API (Ticket: NLP-AC-002).

### 4.2 Interest-bearing entity identification

**Purpose:** Detect entities in the clause that could carry protected interests (e.g., whenua, taonga, cultural knowledge, ecosystems).

**Inputs:**

- Dependency parse outputs (`subject`, `object`, `pobj`, `nmod` spans)
- Named entities tagged as locations, cultural groups, natural features, or resources
- Glossary-backed keywords for interest-bearing entities

**Outputs:**

- Normalised entity candidates with span offsets and hints for interest type (resource, place, knowledge, person/community)
- Provenance: `source = "interest_entity_detector_v1"`

**Action items:**

- Add pattern and embedding-based detectors for interest-bearing entities in `src/nlp` (Ticket: NLP-INT-001).
- Store candidate entities on `RuleAtom` for later linking to `ProtectedInterest` (Ticket: NLP-INT-002).

### 4.3 ProtectedInterest inference

**Purpose:** Map interest-bearing entities to ontology `ProtectedInterest` records and their `ProtectedInterestType`/`ValueDimension` descriptors.

**Inputs:**

- Interest-bearing entity candidates from Â§4.2
- Glossary lookups and ontology tables for interest types
- Contextual clues from conditions/scope and modality (e.g., preservation vs. exploitation)

**Outputs:**

- `protected_interest_id` and `protected_interest_type_id` linked to ontology rows
- Optional `value_dimension_ids` to encode cultural/environmental/economic axes
- Provenance: `source = "protected_interest_inferencer_v1"`

**Action items:**

- Build resolver that links entity candidates to ontology interests using glossary similarity + rule context (Ticket: NLP-PI-001).
- Expose unmatched interests as reviewable tasks with suggested ontology rows (Ticket: NLP-PI-002).

### 4.4 ValueFrame inference

**Purpose:** Infer `ValueFrame` records that express how values (e.g., stewardship, autonomy, equity) are balanced or prioritised in the rule.

**Inputs:**

- Rule modality, conditions, and offence elements (conduct/fault/circumstance)
- Protected interest candidates and actor classes
- Cue phrases indicating value trade-offs or priorities

**Outputs:**

- `value_frame_id` plus scored `value_dimension_ids` indicating the value emphasis
- Provenance: `source = "value_frame_inferencer_v1"`

**Action items:**

- Define cue phrase library and scoring rubric for value dimensions (Ticket: NLP-VF-001).
- Implement inference module that attaches `ValueFrame` candidates to `RuleAtom` records (Ticket: NLP-VF-002).

### 4.5 WrongType candidate inference

**Purpose:** Derive ontology `WrongType` candidates (e.g., offence categories, breaches, duties) from the ruleâ€™s conduct/fault/circumstance structure.

**Inputs:**

- `RuleElement` fragments (conduct, fault, circumstance, exception, result)
- ActorClass and ProtectedInterest outputs
- Offence labels captured during pattern matching

**Outputs:**

- `wrong_type_id` and optional `wrong_subtype`/`category` markers
- Linkage between `WrongType` and triggering `RuleElement` fragments
- Provenance: `source = "wrong_type_inferencer_v1"`

**Action items:**

- Train/encode pattern library mapping offence phrases and conduct structures to `WrongType` taxonomy (Ticket: NLP-WT-001).
- Persist candidate scores and rationales alongside `RuleAtom` for curator review (Ticket: NLP-WT-002).

### 4.6 LegalSource binding

**Purpose:** Bind references detected in Â§2.3 to ontology `LegalSource` records (statutes, cases, treaties, tikanga statements, religious texts).

**Inputs:**

- Normalised references and `reference_source` metadata from the entity ruler
- Provision metadata (work/section identifiers)
- External citation registries where available

**Outputs:**

- `legal_source_id` and optional `norm_source_category_id`
- Anchor metadata (work, section, pinpoint) for cross-linking provisions to ontology sources
- Provenance: `source = "legal_source_binder_v1"`

**Action items:**

- Implement resolver that matches references to `LegalSource` rows using citation text + provision metadata (Ticket: NLP-LS-001).
- Add validation step that flags unresolved or ambiguous sources for manual curation (Ticket: NLP-LS-002).

---

## 5. Storage alignment

Structured RuleAtoms are persisted today; future semantic enrichments should ensure storage of ActorClass, ProtectedInterest, ValueFrame, WrongType, and LegalSource bindings alongside provenance and reviewer status to keep the ontology layer auditable.
